{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tesnime/stagePFE/blob/main/AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7x_bnPIFjRKW",
        "outputId": "665bfab5-08e4-4840-e242-e147e57a87a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.2-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting starlette<0.41.0,>=0.37.2 (from fastapi)\n",
            "  Downloading starlette-0.40.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.9.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.41.0,>=0.37.2->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.41.0,>=0.37.2->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.41.0,>=0.37.2->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.41.0,>=0.37.2->fastapi) (1.2.2)\n",
            "Downloading fastapi-0.115.2-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.40.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyngrok, h11, uvicorn, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.2 h11-0.14.0 pyngrok-7.2.0 starlette-0.40.0 uvicorn-0.32.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro009pr-HDg4",
        "outputId": "97cda6a2-1846-4291-a97e-9aa089bf125c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbqXe2xhlbZh",
        "outputId": "293407ff-8cdd-4209-8290-eb472622758c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m153.6/154.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.13.1)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp310-cp310-linux_x86_64.whl size=2357292 sha256=bd5efda713983acf6a09f08823e827fbc6ff3c40dcbdc559696a652a556b3597\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/3f/df/6acbf0a40397d9bf3ff97f582cc22fb9ce66adde75bc71fd54\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QIo7gL9nNzl",
        "outputId": "93cd85aa-1712-4c87-aafb-a075c70a9a3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tF7yiXtnSMD",
        "outputId": "fdc75440-a891-4153-e3ca-9ab3c45fb858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2mM5nTH7jR1D2Wh35BUlojVlNmG_5P5gEuRsZAp9qxsuRBNz1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkCcJaU2xvto",
        "outputId": "3606a21e-676e-4884-d87d-ea30b5421329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=59759330cdafb1658e3fafba66045b301a78be081bf8901fdac4b5748ab83245\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Collecting phonenumbers\n",
            "  Downloading phonenumbers-8.13.47-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Downloading phonenumbers-8.13.47-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: phonenumbers\n",
            "Successfully installed phonenumbers-8.13.47\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (10.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.2)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n",
            "Collecting fr-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (13.9.2)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.1)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.11-cp38-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.2)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.1)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading PyMuPDF-1.24.11-cp38-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.11\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect\n",
        "!pip install phonenumbers\n",
        "!pip install pdfplumber spacy\n",
        "!python -m spacy download fr_core_news_sm\n",
        "!pip install PyMuPDF spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEbirJab3cBC",
        "outputId": "7c2753d9-5a7c-4f57-cab9-8352919a302f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Downloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: python-multipart\n",
            "Successfully installed python-multipart-0.0.12\n"
          ]
        }
      ],
      "source": [
        "!pip install python-multipart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 619,
          "referenced_widgets": [
            "b2e426d28504414b99e4a5cd65ba7e39",
            "823420a1cbe744499f58fb1a46ff6edf",
            "c2138c0ba4924af687ac2aa8edf21156",
            "466b4a6dcd8542cda5b3ff735c1bb9e7"
          ]
        },
        "id": "lRjzvxlU3VVe",
        "outputId": "e34db33b-9c78-4738-82d6-133e10399dd7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2e426d28504414b99e4a5cd65ba7e39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/841 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "823420a1cbe744499f58fb1a46ff6edf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2138c0ba4924af687ac2aa8edf21156",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "466b4a6dcd8542cda5b3ff735c1bb9e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "<ipython-input-8-ae178f3a4d82>:57: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  evaluation_apprenant_df['star'].fillna(mean_star, inplace=True)\n",
            "INFO:     Started server process [1244]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FastAPI app is running on NgrokTunnel: \"https://7f37-34-91-99-153.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "INFO:     197.0.37.248:0 - \"POST /recommend-themes-by-formateur/?formateur_id=723 HTTP/1.1\" 200 OK\n",
            "INFO:     197.0.37.248:0 - \"POST /check-comment/ HTTP/1.1\" 200 OK\n",
            "INFO:     197.0.37.248:0 - \"POST /check-comment/ HTTP/1.1\" 200 OK\n",
            "Index(['demande_id', 'cv', 'nbr_session', 'theme', 'formateur_id', 'status',\n",
            "       'titre', 'date', 'raison', 'description', 'doc', 'code'],\n",
            "      dtype='object')\n",
            "Formateur Ratings:\n",
            " INFO:     197.0.37.248:0 - \"GET /recommend-themes/ HTTP/1.1\" 200 OK\n",
            "   demande_id  average_star\n",
            "0           5      2.666667\n",
            "1          33      4.000000\n",
            "Formateur Performance:\n",
            "    demande_id  cv  nbr_session  theme  formateur_id    status  \\\n",
            "0           5 NaN            4    NaN           4.0  terminer   \n",
            "1          33 NaN            3    NaN         721.0  terminer   \n",
            "\n",
            "                  titre                        date raison  \\\n",
            "0      titre de demande                         NaN    NaN   \n",
            "1  demande de formation  2024-07-08 10:19:17.763000    NaN   \n",
            "\n",
            "                 description  doc        code  average_star  \n",
            "0  description de la demande  NaN  session123      2.666667  \n",
            "1                     sdfghj  NaN    DLI-0933      4.000000  \n",
            "Best Formateurs:\n",
            "    demande_id  cv  nbr_session  theme  formateur_id    status  \\\n",
            "0           5 NaN            4    NaN           4.0  terminer   \n",
            "1           5 NaN            4    NaN           4.0  terminer   \n",
            "2           5 NaN            4    NaN           4.0  terminer   \n",
            "3           5 NaN            4    NaN           4.0  terminer   \n",
            "4           5 NaN            4    NaN           4.0  terminer   \n",
            "5           5 NaN            4    NaN           4.0  terminer   \n",
            "6           5 NaN            4    NaN           4.0  terminer   \n",
            "\n",
            "              titre date raison                description  doc        code  \\\n",
            "0  titre de demande  NaN    NaN  description de la demande  NaN  session123   \n",
            "1  titre de demande  NaN    NaN  description de la demande  NaN  session123   \n",
            "2  titre de demande  NaN    NaN  description de la demande  NaN  session123   \n",
            "3  titre de demande  NaN    NaN  description de la demande  NaN  session123   \n",
            "4  titre de demande  NaN    NaN  description de la demande  NaN  session123   \n",
            "5  titre de demande  NaN    NaN  description de la demande  NaN  session123   \n",
            "6  titre de demande  NaN    NaN  description de la demande  NaN  session123   \n",
            "\n",
            "   average_star     com_tech  \n",
            "0      2.666667       python  \n",
            "1      2.666667         java  \n",
            "2      2.666667         html  \n",
            "3      2.666667          css  \n",
            "4      2.666667      angular  \n",
            "5      2.666667  spring boot  \n",
            "6      2.666667          sql  \n",
            "INFO:     197.0.37.248:0 - \"GET /best-formateur/ HTTP/1.1\" 200 OK\n",
            "Index(['demande_id', 'cv', 'nbr_session', 'theme', 'formateur_id', 'status',\n",
            "       'titre', 'date', 'raison', 'description', 'doc', 'code'],\n",
            "      dtype='object')\n",
            "Formateur Ratings:\n",
            "    demande_id  average_star\n",
            "0           5      2.666667\n",
            "1          33      4.000000\n",
            "Formateur Performance:\n",
            "    demande_id  cv  nbr_session  theme  formateur_id    status  \\\n",
            "0           5 NaN            4    NaN           4.0  terminer   \n",
            "1          33 NaN            3    NaN         721.0  terminer   \n",
            "\n",
            "                  titre                        date raison  \\\n",
            "0      titre de demande                         NaN    NaN   \n",
            "1  demande de formation  2024-07-08 10:19:17.763000    NaN   \n",
            "\n",
            "                 description  doc        code  average_star  \n",
            "0  description de la demande  NaN  session123      2.666667  \n",
            "1                     sdfghj  NaN    DLI-0933      4.000000  \n",
            "Best Formateurs:\n",
            "    demande_id  cv  nbr_session  theme  formateur_id    status  \\\n",
            "0           5 NaN            4    NaN           4.0  terminer   \n",
            "1           5 NaN            4    NaN           4.0  terminer   \n",
            "2           5 NaN            4    NaN           4.0  terminer   \n",
            "3           5 NaN            4    NaN           4.0  terminer   \n",
            "4           5 NaN            4    NaN           4.0  terminer   \n",
            "5           5 NaN            4    NaN           4.0  terminer   \n",
            "6           5 NaN            4    NaN           4.0  terminer   \n",
            "\n",
            "              titre date raison                description  doc        code  \\\n",
            "0  titre de demande  NaN    NaN  description de la demande  NaN  session123   \n",
            "1  titre de demande  NaN    NaN  description de la demande  NaN  session123   \n",
            "2  titre de demande  NaN    NaN  description de la demande  NaN  session123   \n",
            "3  titre de demande  NaN    NaN  description de la demande  NaN  session123   \n",
            "4  titre de demande  NaN    NaN  description de la demande  NaN  session123   \n",
            "5  titre de demande  NaN    NaN  description de la demande  NaN  session123   \n",
            "6  titre de demande  NaN    NaN  description de la demande  NaN  session123   \n",
            "\n",
            "   average_star     com_tech  \n",
            "0      2.666667       python  \n",
            "1      2.666667         java  \n",
            "2      2.666667         html  \n",
            "3      2.666667          css  \n",
            "4      2.666667      angular  \n",
            "5      2.666667  spring boot  \n",
            "6      2.666667          sql  \n",
            "INFO:     197.0.37.248:0 - \"GET /best-formateur/ HTTP/1.1\" 200 OK\n",
            "INFO:     197.0.37.248:0 - \"GET /recommend-themes/ HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Optional\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from collections import Counter\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "import shutil\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "# Apply nest_asyncio patch\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load the data\n",
        "formateur_df = pd.read_csv('/content/drive/MyDrive/datasets/formateur_com_tech (1).csv')\n",
        "demande_df = pd.read_csv('/content/drive/MyDrive/datasets/demande (1).csv')\n",
        "evaluation_apprenant_df = pd.read_csv('/content/drive/MyDrive/datasets/evaluation_apprenant (1).csv')\n",
        "difficulties_df = pd.read_csv('/content/drive/MyDrive/datasets/interactions_difficultes.csv')\n",
        "interests_df = pd.read_csv('/content/drive/MyDrive/datasets/interactions_centre_interet.csv')\n",
        "demande_theme = pd.read_csv('/content/drive/MyDrive/datasets/demande_theme (2).csv')\n",
        "\n",
        "model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Merge the two dataframes on 'interactions_id'\n",
        "merged_df = pd.merge(difficulties_df, interests_df, on='interactions_id', how='outer')\n",
        "\n",
        "# Group the merged dataframe by 'interactions_id' and aggregate the difficulties and interests into lists\n",
        "grouped_df = merged_df.groupby('interactions_id').agg({\n",
        "    'difficultes': lambda x: list(x.unique()),\n",
        "    'centre_interet': lambda x: list(x.unique())\n",
        "}).reset_index()\n",
        "\n",
        "# Count occurrences of each theme from both difficulties and interests\n",
        "theme_counts = Counter()\n",
        "for index, row in grouped_df.iterrows():\n",
        "    theme_counts.update(row['difficultes'])\n",
        "    theme_counts.update(row['centre_interet'])\n",
        "\n",
        "# Convert counts to a DataFrame for easy viewing and sort by count in descending order\n",
        "theme_counts_df = pd.DataFrame(theme_counts.items(), columns=['Theme', 'Count']).sort_values(by='Count', ascending=False)\n",
        "\n",
        "# Replace missing values in the 'star' column with the mean rating\n",
        "mean_star = evaluation_apprenant_df['star'].mean()\n",
        "evaluation_apprenant_df['star'].fillna(mean_star, inplace=True)\n",
        "\n",
        "# Merge evaluation data with demande to access formateur_id\n",
        "evaluation_with_formateur = pd.merge(\n",
        "    evaluation_apprenant_df,\n",
        "    demande_df[['id', 'formateur_id']],\n",
        "    left_on='id',\n",
        "    right_on='id'\n",
        ")\n",
        "\n",
        "# Group by formateur_id to get the average rating for each trainer\n",
        "grouped_evaluation = evaluation_with_formateur.groupby('formateur_id')['star'].mean().reset_index()\n",
        "\n",
        "# Prepare data for RandomForestRegressor\n",
        "X_formateur = formateur_df[['com_tech']]\n",
        "y_formateur = grouped_evaluation.set_index('formateur_id').reindex(formateur_df['formateur_id'])['star'].dropna()\n",
        "\n",
        "# Remove rows where y_formateur is NaN\n",
        "X_formateur_encoded = pd.get_dummies(X_formateur).reindex(index=y_formateur.index, fill_value=0)\n",
        "\n",
        "# Train the model\n",
        "model_formateur = RandomForestRegressor(random_state=42)\n",
        "model_formateur.fit(X_formateur_encoded, y_formateur)\n",
        "\n",
        "# Prepare for regression-based theme recommendations\n",
        "demande_df.rename(columns={'id': 'demande_id'}, inplace=True)\n",
        "demande_with_scores = pd.merge(demande_df, evaluation_apprenant_df, on='demande_id', how='left')\n",
        "\n",
        "\n",
        "def get_best_formateur():\n",
        "    print(demande_df.columns)\n",
        "    # Group by demande_id and calculate the mean star rating\n",
        "    formateur_ratings = evaluation_apprenant_df.groupby('demande_id')['star'].mean().reset_index()\n",
        "    formateur_ratings.columns = ['demande_id', 'average_star']\n",
        "\n",
        "    print(\"Formateur Ratings:\\n\", formateur_ratings)  # Debug print\n",
        "\n",
        "    # Merge the ratings with the demande dataframe to link it to formateurs\n",
        "    formateur_performance = pd.merge(demande_df, formateur_ratings, left_on='demande_id', right_on='demande_id', how='inner')\n",
        "\n",
        "    if formateur_performance.empty:\n",
        "        print(\"Formateur Performance is empty\")  # Debug print\n",
        "        return None\n",
        "\n",
        "    print(\"Formateur Performance:\\n\", formateur_performance)  # Debug print\n",
        "\n",
        "    # Merge formateur_performance with formateurComp to get formateurs' competencies\n",
        "    best_formateurs = pd.merge(formateur_performance, formateur_df, on='formateur_id', how='inner')\n",
        "\n",
        "    if best_formateurs.empty:\n",
        "        print(\"Best Formateurs is empty\")  # Debug print\n",
        "        return None\n",
        "\n",
        "    print(\"Best Formateurs:\\n\", best_formateurs)  # Debug print\n",
        "\n",
        "    best_formateurs = best_formateurs.sort_values(by='average_star', ascending=False)\n",
        "\n",
        "    best_formateur_id = int(best_formateurs.iloc[0]['formateur_id'])\n",
        "    return best_formateur_id\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to prepare data for regression\n",
        "def prepare_data(formateur_id):\n",
        "    competencies = formateur_df[formateur_df['formateur_id'] == formateur_id]['com_tech'].values\n",
        "    if competencies.size == 0:\n",
        "        return None\n",
        "\n",
        "    relevant_demands = demande_with_scores[demande_with_scores['formateur_id'] == formateur_id]\n",
        "    regression_data = relevant_demands[['demande_id', 'theme', 'star']].copy()\n",
        "\n",
        "    mean_star = regression_data['star'].mean()\n",
        "    regression_data['star'] = regression_data['star'].fillna(mean_star)\n",
        "\n",
        "    if regression_data.empty:\n",
        "        return None\n",
        "\n",
        "    regression_data['competency'] = competencies[0]\n",
        "    return regression_data\n",
        "\n",
        "# Function to recommend themes using regression\n",
        "def recommend_themes_for_formateur(formateur_id):\n",
        "    regression_data = prepare_data(formateur_id)\n",
        "    if regression_data is None:\n",
        "        return []\n",
        "\n",
        "    y = regression_data['star']\n",
        "\n",
        "    theme_encoder = OneHotEncoder(sparse_output=False)\n",
        "    X_theme_encoded = theme_encoder.fit_transform(regression_data[['theme']])\n",
        "\n",
        "    competency_encoder = OneHotEncoder(sparse_output=False)\n",
        "    X_comp_encoded = competency_encoder.fit_transform(regression_data[['competency']])\n",
        "\n",
        "    X_final = pd.DataFrame(X_theme_encoded, columns=theme_encoder.get_feature_names_out(['theme']))\n",
        "    X_comp_final = pd.DataFrame(X_comp_encoded, columns=competency_encoder.get_feature_names_out(['competency']))\n",
        "\n",
        "    X_final = pd.concat([X_final, X_comp_final], axis=1)\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_final, y)\n",
        "\n",
        "    predictions = model.predict(X_final)\n",
        "    regression_data['predicted_score'] = predictions\n",
        "\n",
        "    recommendations = pd.merge(regression_data, demande_theme, on='demande_id', how='left')\n",
        "    recommended_themes = recommendations.sort_values(by='predicted_score', ascending=False)\n",
        "\n",
        "    top_themes = recommended_themes[['theme_y']].drop_duplicates().head(7)\n",
        "    return top_themes['theme_y'].tolist()\n",
        "\n",
        "# Prediction function for suitable trainers by theme\n",
        "def predict_formateur_by_theme(themes: List[str]) -> pd.DataFrame:\n",
        "    themes_set = set(theme.lower().strip() for theme in themes)\n",
        "\n",
        "    def has_matching_comp(com_list):\n",
        "        return any(comp.lower().strip() in themes_set for comp in com_list.split(','))\n",
        "\n",
        "    # Filter formateurs to find matching trainers\n",
        "    filtered_df = formateur_df[formateur_df['com_tech'].apply(has_matching_comp)]\n",
        "\n",
        "    if filtered_df.empty:\n",
        "        return pd.DataFrame(columns=['formateur_id', 'predicted_star'])\n",
        "\n",
        "    # Prepare data for prediction\n",
        "    X_predict = pd.get_dummies(filtered_df[['com_tech']])\n",
        "    X_predict = X_predict.reindex(columns=X_formateur_encoded.columns, fill_value=0)\n",
        "\n",
        "    # Make predictions\n",
        "    filtered_df['predicted_star'] = model_formateur.predict(X_predict)\n",
        "\n",
        "    # Return the top 4 trainers based on predicted_star\n",
        "    top_trainers = filtered_df.nlargest(4, 'predicted_star')\n",
        "\n",
        "    return top_trainers[['formateur_id', 'predicted_star']]\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Pydantic models for input requests\n",
        "class TrainerRequest(BaseModel):\n",
        "    themes: List[str]\n",
        "\n",
        "class FormateurIDRequest(BaseModel):\n",
        "    formateur_id: int\n",
        "\n",
        "class CommentRequest(BaseModel):\n",
        "    comment: str  # Single comment\n",
        "\n",
        "# Function to check toxicity of a comment\n",
        "def check_comment(comment: str) -> bool:\n",
        "    results = classifier(comment)\n",
        "    for result in results:\n",
        "        if result['label'] == 'negative' and result['score'] > 0.5:  # Toxicity threshold\n",
        "            return False  # Inappropriate comment\n",
        "    return True  # Acceptable comment\n",
        "\n",
        "# class CVData(BaseModel):\n",
        "#     name: Optional[str]\n",
        "#     position: Optional[str]\n",
        "#     profile: Optional[str]\n",
        "#     experience: List[str]\n",
        "#     education: List[str]\n",
        "#     certifications: Optional[str]\n",
        "#     contact: Optional[str]\n",
        "#     skills: List[str]\n",
        "#     languages: Optional[str]\n",
        "#     interests: Optional[str]\n",
        "#     email: Optional[str]\n",
        "#     address: Optional[str]\n",
        "\n",
        "# def extract_cv_data(pdf_path):\n",
        "#     # Your existing extraction logic here\n",
        "#     doc = fitz.open(pdf_path)\n",
        "#     text = \"\"\n",
        "#     for page_num in range(len(doc)):\n",
        "#         page = doc.load_page(page_num)\n",
        "#         text += page.get_text()\n",
        "\n",
        "#     data = {\n",
        "#         'name': extract_info(text, 'Nom Prénom', 'Intitulé du poste'),\n",
        "#         'position': extract_info(text, 'Intitulé du poste', 'Profil Professionnel'),\n",
        "#         'profile': clean_profile(extract_info(text, 'Profil Professionnel', 'Expérience Professionnelle')),\n",
        "#         'experience': split_experience(extract_info(text, 'Expérience Professionnelle', 'Formation')),\n",
        "#         'education': split_education(extract_info(text, 'Formation', 'Certifications')),\n",
        "#         'certifications': extract_info(text, 'Certifications', 'Contact'),\n",
        "#         'contact': clean_contact(extract_info(text, 'Contact', 'Compétences')),\n",
        "#         'skills': split_skills(extract_info(text, 'Compétences', 'Langues')),\n",
        "#         'languages': extract_info(text, 'Langues', 'Centres d\\'intérêt'),\n",
        "#         'interests': extract_info(text, 'Centres d\\'intérêt', 'Certificat d\\'Aptitude'),\n",
        "#     }\n",
        "\n",
        "#     email, address = extract_email_and_address(text)\n",
        "#     data['email'] = email\n",
        "#     data['address'] = address\n",
        "\n",
        "#     return data\n",
        "\n",
        "# @app.post(\"/upload_cv\", response_model=CVData)\n",
        "# async def upload_cv(file: UploadFile = File(...)):\n",
        "#     # Save the uploaded file to a temporary location\n",
        "#     pdf_path = f\"/tmp/{file.filename}\"\n",
        "\n",
        "#     with open(pdf_path, \"wb\") as buffer:\n",
        "#         buffer.write(await file.read())\n",
        "\n",
        "#     try:\n",
        "#         # Extract CV data from the uploaded PDF\n",
        "#         cv_data = extract_cv_data(pdf_path)\n",
        "#     finally:\n",
        "#         # Clean up the temporary file\n",
        "#         os.remove(pdf_path)\n",
        "\n",
        "#     return cv_data\n",
        "\n",
        "\n",
        "\n",
        "class CVData(BaseModel):\n",
        "    name: Optional[str]\n",
        "    position: Optional[str]\n",
        "    profile: Optional[str]\n",
        "    experience: List[str]\n",
        "    education: List[str]\n",
        "    certifications: Optional[str]\n",
        "    contact: Optional[str]\n",
        "    skills: List[str]\n",
        "    languages: Optional[str]\n",
        "    interests: Optional[str]\n",
        "    email: Optional[str]\n",
        "    address: Optional[str]\n",
        "    phone: Optional[str]\n",
        "\n",
        "\n",
        "def extract_phone_using_nlp(text):\n",
        "    \"\"\"Extract phone numbers using NLP techniques.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    phone_numbers = []\n",
        "\n",
        "    for token in doc:\n",
        "        # Simple heuristic: check for tokens that might represent phone numbers\n",
        "        if token.like_num and (len(token.text) >= 7 and len(token.text) <= 15):\n",
        "            # Assume tokens with certain lengths are part of a phone number\n",
        "            phone_numbers.append(token.text)\n",
        "\n",
        "    # Join the found tokens to form a potential phone number\n",
        "    if phone_numbers:\n",
        "        return \" \".join(phone_numbers)\n",
        "    return None\n",
        "\n",
        "def extract_info(text, start_marker, end_marker):\n",
        "    \"\"\"Extract information between two markers in the text.\"\"\"\n",
        "    start_index = text.find(start_marker)\n",
        "    end_index = text.find(end_marker, start_index)\n",
        "    if start_index != -1 and end_index != -1:\n",
        "        return text[start_index + len(start_marker):end_index].strip()\n",
        "    return None\n",
        "\n",
        "def extract_position_using_nlp(text):\n",
        "    \"\"\"Extract position title using NLP.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    position = None\n",
        "\n",
        "    # Simple heuristic: looking for phrases that typically indicate a position title\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"nsubj\" and token.pos_ == \"PROPN\":\n",
        "            position = token.text\n",
        "            break\n",
        "\n",
        "    # Vous pouvez affiner cette logique pour mieux détecter les intitulés de poste\n",
        "    return position\n",
        "\n",
        "def clean_profile(profile_text):\n",
        "    \"\"\"Clean and format the profile section.\"\"\"\n",
        "    if profile_text:\n",
        "        return profile_text.strip()\n",
        "    return None\n",
        "\n",
        "def split_experience(experience_text):\n",
        "    \"\"\"Split the experience section into a list of experiences.\"\"\"\n",
        "    if experience_text:\n",
        "        return [exp.strip() for exp in experience_text.split(\"\\n\") if exp.strip()]\n",
        "    return []\n",
        "\n",
        "def split_education(education_text):\n",
        "    \"\"\"Split the education section into a list of education entries.\"\"\"\n",
        "    if education_text:\n",
        "        return [edu.strip() for edu in education_text.split(\"\\n\") if edu.strip()]\n",
        "    return []\n",
        "\n",
        "def clean_contact(contact_text):\n",
        "    \"\"\"Clean and format the contact section.\"\"\"\n",
        "    if contact_text:\n",
        "        return contact_text.strip()\n",
        "    return None\n",
        "\n",
        "def split_skills(skills_text):\n",
        "    \"\"\"Split the skills section into a list of skills.\"\"\"\n",
        "    if skills_text:\n",
        "        return [skill.strip() for skill in skills_text.split(\",\") if skill.strip()]\n",
        "    return []\n",
        "\n",
        "def extract_email_and_address(text):\n",
        "    \"\"\"Uses spaCy to extract email and address from the text.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    email = None\n",
        "    address = \"\"\n",
        "\n",
        "    # Find email\n",
        "    for token in doc:\n",
        "        if token.like_email:\n",
        "            email = token.text\n",
        "            break\n",
        "\n",
        "    # Find address by looking for keywords like street numbers, names, and postal codes\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"GPE\" or ent.label_ == \"LOC\" or ent.label_ == \"FAC\" or ent.label_ == \"ORG\" or ent.label_ == \"ADDRESS\":\n",
        "            if \"Rue\" in ent.text or \"Avenue\" in ent.text or any(char.isdigit() for char in ent.text):\n",
        "                address += ent.text + \" \"\n",
        "\n",
        "    # Cleaning up the address to ensure it is coherent\n",
        "    address = address.strip()\n",
        "\n",
        "    return email, address\n",
        "\n",
        "def extract_cv_data(pdf_path):\n",
        "    # Open the PDF and extract text\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)\n",
        "        text += page.get_text()\n",
        "\n",
        "    # Extract data based on sections\n",
        "    data = {\n",
        "        'name': extract_info(text, 'Nom Prénom', 'Intitulé du poste'),\n",
        "        'position': extract_position_using_nlp(text),\n",
        "        'profile': clean_profile(extract_info(text, 'Profil Professionnel', 'Expérience Professionnelle')),\n",
        "        'experience': split_experience(extract_info(text, 'Expérience Professionnelle', 'Formation')),\n",
        "        'education': split_education(extract_info(text, 'Formation', 'Certifications')),\n",
        "        'certifications': extract_info(text, 'Certifications', 'Contact'),\n",
        "        'contact': clean_contact(extract_info(text, 'Contact', 'Compétences')),\n",
        "        'skills': split_skills(extract_info(text, 'Compétences', 'Langues')),\n",
        "        'languages': extract_info(text, 'Langues', 'Centres d\\'intérêt'),\n",
        "        'interests': extract_info(text, 'Centres d\\'intérêt', 'Certificat d\\'Aptitude'),\n",
        "    }\n",
        "\n",
        "    # Extract email and address separately\n",
        "    email, address = extract_email_and_address(text)\n",
        "    phone = extract_position_using_nlp(text)\n",
        "    data['email'] = email\n",
        "    data['address'] = address\n",
        "    data['phone'] = phone\n",
        "\n",
        "    return CVData(**data)\n",
        "\n",
        "@app.post(\"/upload_cv\", response_model=CVData)\n",
        "async def upload_cv(file: UploadFile = File(...)):\n",
        "    # Save the uploaded file to a temporary location\n",
        "    pdf_path = f\"/tmp/{file.filename}\"\n",
        "\n",
        "    try:\n",
        "        # Open the file in write-binary mode\n",
        "        with open(pdf_path, \"wb\") as buffer:\n",
        "            shutil.copyfileobj(file.file, buffer)\n",
        "\n",
        "        # Extract CV data from the uploaded PDF\n",
        "        cv_data = extract_cv_data(pdf_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"An error occurred while processing the file: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Clean up the temporary file\n",
        "        if os.path.exists(pdf_path):\n",
        "            os.remove(pdf_path)\n",
        "\n",
        "    return cv_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @app.post(\"/upload_cv\")\n",
        "# async def upload_cv(file: UploadFile = File(...)):\n",
        "#     content = await file.read()  # Read the file content\n",
        "#     return {\"filename\": file.filename, \"content_length\": len(content)}\n",
        "\n",
        "# FastAPI route to check a single comment for toxicity\n",
        "@app.post(\"/check-comment/\")\n",
        "def check_single_comment(request: CommentRequest):\n",
        "    comment_status = check_comment(request.comment)\n",
        "    return comment_status\n",
        "\n",
        "# Define the FastAPI route for finding suitable trainers\n",
        "@app.post(\"/suitable-trainers/\")\n",
        "def find_suitable_trainers(request: TrainerRequest):\n",
        "    trainers = predict_formateur_by_theme(request.themes)\n",
        "    return {\"suitable_trainers\": trainers.to_dict(orient='records')}\n",
        "\n",
        "\n",
        "# Define the FastAPI route for recommending themes based on a formateur\n",
        "@app.post(\"/recommend-themes-by-formateur/\")\n",
        "def recommend_themes(formateur_id: int):\n",
        "    recommended_themes = recommend_themes_for_formateur(formateur_id)\n",
        "    return {\"recommended_themes\": recommended_themes}\n",
        "\n",
        "# Define the FastAPI route for recommending popular themes\n",
        "@app.get(\"/recommend-themes/\")\n",
        "def recommend_themes():\n",
        "    recommended_themes = theme_counts_df.nlargest(5, 'Count')['Theme'].tolist()\n",
        "    return {\"recommended_themes\": recommended_themes}\n",
        "\n",
        "\n",
        "@app.get(\"/best-formateur/\")\n",
        "def get_best_formateur_endpoint():\n",
        "    best_formateur_id = get_best_formateur()\n",
        "    if best_formateur_id:\n",
        "        return {\"best_formateur_id\": best_formateur_id}\n",
        "    else:\n",
        "        return {\"message\": \"No formateur data available\"}\n",
        "# Create a public URL using ngrok\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"FastAPI app is running on {public_url}\")\n",
        "\n",
        "# Run the FastAPI app with Uvicorn\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1CznVoZhEaIvRGYQh02_WK3KdpR2T5N24",
      "authorship_tag": "ABX9TyMHcQ0MoWtRtgURk35kLsRf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}